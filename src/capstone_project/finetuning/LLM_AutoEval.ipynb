{
<<<<<<< HEAD
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
=======
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      3\u001b[0m load_dotenv() \u001b[38;5;66;03m# This loads the .env file at the project root\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv() # This loads the .env file at the project root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elyxjYI_rY5W",
        "outputId": "9d80047d-2ef1-4e8f-b46a-3f8dc63db31c"
      },
      "outputs": [
        {
          "ename": "QueryError",
          "evalue": "There are no longer any instances available with the requested specifications. Please refresh and try again.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mQueryError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 52\u001b[0m\n\u001b[1;32m     47\u001b[0m runpod\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRUNPOD_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# GITHUB_API_TOKEN = userdata.get(GITHUB_TOKEN)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# HUGGINGFACE_TOKEN = userdata.get(HF_TOKEN)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Create a pod\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m pod \u001b[38;5;241m=\u001b[39m \u001b[43mrunpod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_pod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEval \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_ID\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m on \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBENCHMARK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapitalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunpod/pytorch:2.0.1-py3.10-cuda11.8.0-devel-ubuntu22.04\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_type_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGPU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloud_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLOUD_TYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUMBER_OF_GPUS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolume_in_gb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_disk_in_gb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONTAINER_DISK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemplate_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mau6nz6emhk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBENCHMARK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mBENCHMARK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMODEL_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mREPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mREPO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTRUST_REMOTE_CODE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRUST_REMOTE_CODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPRIVATE_GIST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPRIVATE_GIST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDEBUG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEBUG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGITHUB_API_TOKEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mGITHUB_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHUGGINGFACE_TOKEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLIGHT_EVAL_TASK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mLIGHTEVAL_TASK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNUMBER_OF_GPUS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUMBER_OF_GPUS\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPod started: https://www.runpod.io/console/pods\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/runpod/api/ctl_commands.py:147\u001b[0m, in \u001b[0;36mcreate_pod\u001b[0;34m(name, image_name, gpu_type_id, cloud_type, support_public_ip, start_ssh, data_center_id, country_code, gpu_count, volume_in_gb, container_disk_in_gb, min_vcpu_count, min_memory_in_gb, docker_args, ports, volume_mount_path, env, template_id, network_volume_id, allowed_cuda_versions)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m container_disk_in_gb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m template_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     container_disk_in_gb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m--> 147\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m \u001b[43mrun_graphql_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpod_mutations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_pod_deployment_mutation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpu_type_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcloud_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_public_ip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_ssh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_center_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcountry_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpu_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvolume_in_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontainer_disk_in_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_vcpu_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_memory_in_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocker_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvolume_mount_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemplate_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnetwork_volume_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowed_cuda_versions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m cleaned_response \u001b[38;5;241m=\u001b[39m raw_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpodFindAndDeployOnDemand\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_response\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/runpod/api/graphql.py:37\u001b[0m, in \u001b[0;36mrun_graphql_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mAuthenticationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnauthorized request, please check your API key.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson():\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mQueryError(\n\u001b[1;32m     38\u001b[0m         response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     39\u001b[0m         query\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "\u001b[0;31mQueryError\u001b[0m: There are no longer any instances available with the requested specifications. Please refresh and try again."
          ]
        }
      ],
>>>>>>> 832f027381325b1915686afccf2fdba8edb49ce6
      "source": [
        "# @title # 🧐 LLM AutoEval\n",
        "\n",
        "# @markdown > 🗣️ [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "# @markdown ❤️ Created by [@maximelabonne](https://twitter.com/maximelabonne).\n",
        "# @markdown * This notebook allows you to **automatically evaluate your LLMs** using RunPod (please consider using my [referral link](https://runpod.io?ref=9nvk2srl)).\n",
        "# @markdown * The results are automatically uploaded to [GitHub Gist](https://gist.github.com/) and the pod is destroyed (you can safely close this tab).\n",
        "# @markdown * For further details, see the project on 💻 [GitHub](https://github.com/mlabonne/llm-autoeval).\n",
        "\n",
        "!pip install -qqq runpod --progress-bar off\n",
        "\n",
        "import runpod\n",
<<<<<<< HEAD
        "from google.colab import userdata\n",
=======
        "# from google.colab import userdata\n",
>>>>>>> 832f027381325b1915686afccf2fdba8edb49ce6
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## 🔍 Evaluation\n",
        "MODEL_ID = \"https://huggingface.co/Haleshot/Mathmate-7B-DELLA\" # @param {type:\"string\"}\n",
        "BENCHMARK = \"nous\" # @param [\"nous\", \"eq-bench\", \"openllm\", \"lighteval\"]\n",
        "\n",
        "# @markdown For lighteval, select tasks as specified in the [readme](https://github.com/huggingface/lighteval?tab=readme-ov-file#usage) or in the list of [recommended tasks](https://github.com/huggingface/lighteval/blob/main/tasks_examples/recommended_set.txt).\n",
        "\n",
        "LIGHTEVAL_TASK = \"leaderboard|truthfulqa:mc|0|0,leaderboard|gsm8k|0|0\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## ☁️ Cloud GPU\n",
        "\n",
<<<<<<< HEAD
        "GPU = \"NVIDIA GeForce RTX 3090\" # @param [\"NVIDIA A100 80GB PCIe\", \"NVIDIA A100-SXM4-80GB\", \"NVIDIA A30\", \"NVIDIA A40\", \"NVIDIA GeForce RTX 3070\", \"NVIDIA GeForce RTX 3080\", \"NVIDIA GeForce RTX 3080 Ti\", \"NVIDIA GeForce RTX 3090\", \"NVIDIA GeForce RTX 3090 Ti\", \"NVIDIA GeForce RTX 4070 Ti\", \"NVIDIA GeForce RTX 4080\", \"NVIDIA GeForce RTX 4090\", \"NVIDIA H100 80GB HBM3\", \"NVIDIA H100 PCIe\", \"NVIDIA L4\", \"NVIDIA L40\", \"NVIDIA RTX 4000 Ada Generation\", \"NVIDIA RTX 4000 SFF Ada Generation\", \"NVIDIA RTX 5000 Ada Generation\", \"NVIDIA RTX 6000 Ada Generation\", \"NVIDIA RTX A2000\", \"NVIDIA RTX A4000\", \"NVIDIA RTX A4500\", \"NVIDIA RTX A5000\", \"NVIDIA RTX A6000\", \"Tesla V100-FHHL-16GB\", \"Tesla V100-PCIE-16GB\", \"Tesla V100-SXM2-16GB\", \"Tesla V100-SXM2-32GB\"]\n",
=======
        "GPU = \"NVIDIA L4\" # @param [\"NVIDIA A100 80GB PCIe\", \"NVIDIA A100-SXM4-80GB\", \"NVIDIA A30\", \"NVIDIA A40\", \"NVIDIA GeForce RTX 3070\", \"NVIDIA GeForce RTX 3080\", \"NVIDIA GeForce RTX 3080 Ti\", \"NVIDIA GeForce RTX 3090\", \"NVIDIA GeForce RTX 3090 Ti\", \"NVIDIA GeForce RTX 4070 Ti\", \"NVIDIA GeForce RTX 4080\", \"NVIDIA GeForce RTX 4090\", \"NVIDIA H100 80GB HBM3\", \"NVIDIA H100 PCIe\", \"NVIDIA L4\", \"NVIDIA L40\", \"NVIDIA RTX 4000 Ada Generation\", \"NVIDIA RTX 4000 SFF Ada Generation\", \"NVIDIA RTX 5000 Ada Generation\", \"NVIDIA RTX 6000 Ada Generation\", \"NVIDIA RTX A2000\", \"NVIDIA RTX A4000\", \"NVIDIA RTX A4500\", \"NVIDIA RTX A5000\", \"NVIDIA RTX A6000\", \"Tesla V100-FHHL-16GB\", \"Tesla V100-PCIE-16GB\", \"Tesla V100-SXM2-16GB\", \"Tesla V100-SXM2-32GB\"]\n",
>>>>>>> 832f027381325b1915686afccf2fdba8edb49ce6
        "NUMBER_OF_GPUS = 1 # @param {type:\"slider\", min:1, max:8, step:1}\n",
        "CONTAINER_DISK = 75 # @param {type:\"slider\", min:50, max:500, step:25}\n",
        "CLOUD_TYPE = \"COMMUNITY\" # @param [\"COMMUNITY\", \"SECURE\"]\n",
        "REPO = \"https://github.com/mlabonne/llm-autoeval.git\" # @param {type:\"string\"}\n",
<<<<<<< HEAD
        "TRUST_REMOTE_CODE = False # @param {type:\"boolean\"}\n",
        "PRIVATE_GIST = True # @param {type:\"boolean\"}\n",
        "DEBUG = False # @param {type:\"boolean\"}\n",
=======
        "TRUST_REMOTE_CODE = True # @param {type:\"boolean\"}\n",
        "PRIVATE_GIST = True # @param {type:\"boolean\"}\n",
        "DEBUG = True # @param {type:\"boolean\"}\n",
>>>>>>> 832f027381325b1915686afccf2fdba8edb49ce6
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## 🔑 Tokens\n",
        "# @markdown Enter the name of your tokens in the Secrets tab.\n",
<<<<<<< HEAD
        "RUNPOD_TOKEN = \"runpod\" # @param {type:\"string\"}\n",
        "GITHUB_TOKEN = \"github\" # @param {type:\"string\"}\n",
        "HF_TOKEN = \"HF_TOKEN\" # @param {type:\"string\"}\n",
        "\n",
        "# Environment variables\n",
        "runpod.api_key = userdata.get(RUNPOD_TOKEN)\n",
        "GITHUB_API_TOKEN = userdata.get(GITHUB_TOKEN)\n",
        "HUGGINGFACE_TOKEN = userdata.get(HF_TOKEN)\n",
=======
        "# RUNPOD_TOKEN = os.getenv('RUNPOD_TOKEN') # @param {type:\"string\"}\n",
        "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN') # @param {type:\"string\"}\n",
        "HF_TOKEN = os.getenv('HF_TOKEN') # @param {type:\"string\"}\n",
        "\n",
        "# Environment variables\n",
        "runpod.api_key = os.getenv('RUNPOD_TOKEN')\n",
        "# GITHUB_API_TOKEN = userdata.get(GITHUB_TOKEN)\n",
        "# HUGGINGFACE_TOKEN = userdata.get(HF_TOKEN)\n",
>>>>>>> 832f027381325b1915686afccf2fdba8edb49ce6
        "\n",
        "# Create a pod\n",
        "pod = runpod.create_pod(\n",
        "    name=f\"Eval {MODEL_ID.split('/')[-1]} on {BENCHMARK.capitalize()}\",\n",
        "    image_name=\"runpod/pytorch:2.0.1-py3.10-cuda11.8.0-devel-ubuntu22.04\",\n",
        "    gpu_type_id=GPU,\n",
        "    cloud_type=CLOUD_TYPE,\n",
        "    gpu_count=NUMBER_OF_GPUS,\n",
        "    volume_in_gb=0,\n",
        "    container_disk_in_gb=CONTAINER_DISK,\n",
        "    template_id=\"au6nz6emhk\",\n",
        "    env={\n",
        "        \"BENCHMARK\": BENCHMARK,\n",
        "        \"MODEL_ID\": MODEL_ID,\n",
        "        \"REPO\": REPO,\n",
        "        \"TRUST_REMOTE_CODE\": TRUST_REMOTE_CODE,\n",
        "        \"PRIVATE_GIST\": PRIVATE_GIST,\n",
        "        \"DEBUG\": DEBUG,\n",
<<<<<<< HEAD
        "        \"GITHUB_API_TOKEN\": GITHUB_API_TOKEN,\n",
        "        \"HUGGINGFACE_TOKEN\": HUGGINGFACE_TOKEN,\n",
=======
        "        \"GITHUB_API_TOKEN\": GITHUB_TOKEN,\n",
        "        \"HUGGINGFACE_TOKEN\": HF_TOKEN,\n",
>>>>>>> 832f027381325b1915686afccf2fdba8edb49ce6
        "        \"LIGHT_EVAL_TASK\": LIGHTEVAL_TASK,\n",
        "        \"NUMBER_OF_GPUS\": NUMBER_OF_GPUS\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Pod started: https://www.runpod.io/console/pods\")"
<<<<<<< HEAD
      ],
      "metadata": {
        "id": "elyxjYI_rY5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d80047d-2ef1-4e8f-b46a-3f8dc63db31c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pod started: https://www.runpod.io/console/pods\n"
          ]
        }
      ]
    }
  ]
}
=======
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
>>>>>>> 832f027381325b1915686afccf2fdba8edb49ce6
