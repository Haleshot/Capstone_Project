{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wq4SB9A_9ic"
      },
      "source": [
        "# ü•± LazyMergekit\n",
        "\n",
        "> üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne).\n",
        "\n",
        "This notebook allows you to easily merge multiple models using [mergekit](https://github.com/cg123/mergekit). To evaluate your merges, see [üßê LLM AutoEval](https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing#scrollTo=elyxjYI_rY5W).\n",
        "\n",
        "*Special thanks to [@cg123](https://github.com/cg123) for this library and [@mrfakename](https://gist.github.com/fakerybakery) who told me about sharding (see his [Gist](https://gist.github.com/fakerybakery/d30a4d31b4f914757c1381166b9c683b)).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import lightning.pytorch as pl\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LGd7jlfCpNcg"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"Mathmate-7B-moe\"\n",
        "yaml_config = \"\"\"\n",
        "base_model: AI-MO/NuminaMath-7B-TIR\n",
        "merge_method: della # as moe seems to take franken-moe/passthrough as default\n",
        "gate_mode: hidden\n",
        "dtype: bfloat16\n",
        "experts:\n",
        "  - source_model: AI-MO/NuminaMath-7B-TIR\n",
        "    positive_prompts:\n",
        "      - \"This model is good at solving math questions at high school level and generating python code for the same\"\n",
        "  # - source_model: Qwen/Qwen2-Math-7B-Instruct\n",
        "  #   positive_prompts:\n",
        "  #     - \"This model is really good at solving college level math to olympiad level questions\"\n",
        "  - source_model: deepseek-ai/DeepSeek-Prover-V1.5-RL\n",
        "    positive_prompts:\n",
        "      - \"This model is good at formal theorem providing math problems\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# base_model: \"/teamspace/uploads/NuminaMath-7B-TIR.q6_k.gguf\"\n",
        "# gate_mode: hidden # one of \"hidden\", \"cheap_embed\", or \"random\"\n",
        "# dtype: bfloat16 # output dtype (float32, float16, or bfloat16)\n",
        "# ## (optional)\n",
        "# # experts_per_token: 2\n",
        "# experts:\n",
        "#   - source_model: Qwen/Qwen2-Math-7B-Instruct\n",
        "#     positive_prompts:\n",
        "#       - \"This is a prompt that is demonstrative of what expert_model_1 excels at\"\n",
        "#     ## (optional)\n",
        "#     # negative_prompts:\n",
        "#     #   - \"This is a prompt expert_model_1 should not be used for\"\n",
        "#   - source_model: AI-MO/NuminaMath-7B-TIR\n",
        "#   # ... and so on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "id": "d5mYzDo1q96y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mergekit' already exists and is not an empty directory.\n",
            "\n",
            "\n",
            "\u001b[38;5;57m\u001b[1m‚ö°Ô∏è Tip\u001b[0m\tCheck organization access: \u001b[4mhttps://github.com/settings/connections/applications/c7457225b242a94d60c6\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mergekit-moe config.yaml merge --copy-tokenizer --device cuda --low-cpu-memory --trust-remote-code\n",
            "Warm up loaders:   0%|                                    | 0/3 [00:00<?, ?it/s]\n",
            "Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 100047.62it/s]\u001b[A\n",
            "\n",
            "Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 252434.96it/s]\u001b[A\n",
            "Warm up loaders:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 2/3 [00:00<00:00, 17.75it/s]\n",
            "Fetching 6 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 147168.56it/s]\u001b[A\n",
            "Warm up loaders: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 18.57it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:54<00:00,  6.07s/it]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.59s/it]\n",
            "expert prompts:   0%|                                     | 0/2 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "expert prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.23it/s]\n"
          ]
        }
      ],
      "source": [
        "# @title ## Run merge\n",
        "\n",
        "# @markdown ### Runtime type\n",
        "# @markdown Select your runtime (CPU, High RAM, GPU)\n",
        "\n",
        "runtime = \"GPU\" # @param [\"CPU\", \"CPU + High-RAM\", \"GPU\"]\n",
        "\n",
        "# @markdown ### Mergekit arguments\n",
        "# @markdown Use the `main` branch by default, [`mixtral`](https://github.com/cg123/mergekit/blob/mixtral/moe.md) if you want to create a Mixture of Experts.\n",
        "\n",
        "branch = \"mixtral\" # @param [\"main\", \"mixtral\"]\n",
        "trust_remote_code = True # @param {type:\"boolean\"}\n",
        "\n",
        "# Install mergekit\n",
        "if branch == \"main\":\n",
        "    !git clone https://github.com/arcee-ai/mergekit.git\n",
        "    !cd mergekit && pip install -qqq -e . --progress-bar off\n",
        "elif branch == \"mixtral\":\n",
        "    !git clone -b mixtral https://github.com/arcee-ai/mergekit.git\n",
        "    !cd mergekit && pip install -qqq -e . --progress-bar off\n",
        "    !pip install -qqq -U transformers --progress-bar off\n",
        "\n",
        "# Save config as yaml file\n",
        "with open('config.yaml', 'w', encoding=\"utf-8\") as f:\n",
        "    f.write(yaml_config)\n",
        "\n",
        "# Base CLI\n",
        "if branch == \"main\":\n",
        "    cli = \"mergekit-yaml config.yaml merge --copy-tokenizer\"\n",
        "elif branch == \"mixtral\":\n",
        "    cli = \"mergekit-moe config.yaml merge --copy-tokenizer\"\n",
        "\n",
        "# Additional arguments\n",
        "if runtime == \"CPU\":\n",
        "    cli += \" --allow-crimes --out-shard-size 1B --lazy-unpickle\"\n",
        "elif runtime == \"GPU\":\n",
        "    cli += \" --device cuda --low-cpu-memory\"\n",
        "if trust_remote_code:\n",
        "    cli += \" --trust-remote-code\"\n",
        "\n",
        "print(cli)\n",
        "\n",
        "# Merge models\n",
        "!{cli}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhaleshot\u001b[0m (\u001b[33mhaleshot-SVKM's Narsee Monjee Institute of Management St\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/Capstone_Project/wandb/run-20240825_162651-69xavjzm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning/runs/69xavjzm' target=\"_blank\">Mathmate-7B-dare-ties</a></strong> to <a href='https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning' target=\"_blank\">https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning/runs/69xavjzm' target=\"_blank\">https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning/runs/69xavjzm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"
          ]
        }
      ],
      "source": [
        "# Initialize WandB\n",
        "wandb.init(project='Mathmate-stage1-finetuning', name=MODEL_NAME)\n",
        "\n",
        "# Set up WandbLogger\n",
        "wandb_logger = WandbLogger(project='my-merge-project')\n",
        "\n",
        "# Optional: Add any hyperparameters or configuration settings\n",
        "wandb_logger.experiment.config.update({\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"runtime\": runtime,\n",
        "    \"branch\": branch,\n",
        "    # Add any other hyperparameters or settings you want to log\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    logger=wandb_logger,\n",
        "    # Include any other trainer arguments here\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ik0V0dF55gfU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model-00001-of-00003.safetensors:   0%|          | 0.00/9.97G [00:00<?, ?B/s]\n",
            "\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0%|          | 16.0M/9.97G [00:00<02:05, 79.7MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   0%|          | 32.0M/9.97G [00:00<02:09, 76.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   0%|          | 48.0M/9.97G [00:00<02:11, 75.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   1%|          | 64.0M/9.97G [00:00<02:05, 79.1MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   1%|          | 80.0M/9.97G [00:01<02:10, 75.9MB/s]\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   1%|          | 112M/9.97G [00:06<13:35, 12.1MB/s] \n",
            "\n",
            "model-00001-of-00003.safetensors:   1%|‚ñè         | 128M/9.97G [00:06<09:49, 16.7MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   1%|‚ñè         | 144M/9.97G [00:06<07:20, 22.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   2%|‚ñè         | 160M/9.97G [00:06<05:35, 29.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   2%|‚ñè         | 176M/9.97G [00:07<04:38, 35.2MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   2%|‚ñè         | 192M/9.97G [00:07<04:00, 40.6MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   2%|‚ñè         | 208M/9.97G [00:07<03:27, 47.0MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   3%|‚ñé         | 256M/9.97G [00:08<02:35, 62.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   3%|‚ñé         | 272M/9.97G [00:08<02:41, 60.1MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   3%|‚ñé         | 288M/9.97G [00:08<02:45, 58.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   3%|‚ñé         | 304M/9.97G [00:09<02:37, 61.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   3%|‚ñé         | 320M/9.97G [00:09<02:40, 60.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   3%|‚ñé         | 336M/9.97G [00:09<02:43, 58.9MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   4%|‚ñé         | 352M/9.97G [00:09<02:44, 58.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   5%|‚ñç         | 480M/9.97G [00:12<02:28, 63.9MB/s]\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   5%|‚ñå         | 512M/9.97G [00:12<02:24, 65.3MB/s]\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   5%|‚ñå         | 528M/9.97G [00:12<02:26, 64.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   5%|‚ñå         | 544M/9.97G [00:12<02:15, 69.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   6%|‚ñå         | 560M/9.97G [00:13<02:15, 69.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   6%|‚ñå         | 576M/9.97G [00:13<02:12, 71.2MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   6%|‚ñå         | 592M/9.97G [00:13<02:12, 70.7MB/s]\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   6%|‚ñå         | 608M/9.97G [00:18<16:41, 9.35MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   6%|‚ñã         | 624M/9.97G [00:19<12:21, 12.6MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   6%|‚ñã         | 640M/9.97G [00:19<09:08, 17.0MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   7%|‚ñã         | 672M/9.97G [00:19<05:22, 28.8MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   7%|‚ñã         | 688M/9.97G [00:19<04:16, 36.2MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   7%|‚ñã         | 704M/9.97G [00:19<03:35, 43.1MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   7%|‚ñã         | 720M/9.97G [00:20<03:06, 49.6MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   7%|‚ñã         | 736M/9.97G [00:20<02:57, 52.1MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   8%|‚ñä         | 752M/9.97G [00:20<02:42, 56.9MB/s]\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   8%|‚ñä         | 768M/9.97G [00:20<02:33, 59.9MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   8%|‚ñä         | 784M/9.97G [00:21<02:27, 62.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   8%|‚ñä         | 800M/9.97G [00:21<02:25, 62.9MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   8%|‚ñä         | 832M/9.97G [00:21<02:08, 71.0MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   9%|‚ñä         | 848M/9.97G [00:22<02:14, 68.0MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   9%|‚ñä         | 864M/9.97G [00:22<02:20, 64.6MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   9%|‚ñâ         | 880M/9.97G [00:22<02:14, 67.7MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   9%|‚ñâ         | 912M/9.97G [00:23<02:20, 64.6MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   9%|‚ñâ         | 928M/9.97G [00:23<02:13, 67.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:   9%|‚ñâ         | 944M/9.97G [00:23<02:12, 68.4MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  10%|‚ñâ         | 960M/9.97G [00:23<02:03, 73.2MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  10%|‚ñâ         | 976M/9.97G [00:23<02:15, 66.6MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  10%|‚ñà         | 1.01G/9.97G [00:24<02:00, 74.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  10%|‚ñà         | 1.02G/9.97G [00:24<01:55, 77.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  13%|‚ñà‚ñé        | 1.30G/9.97G [00:28<02:20, 61.6MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  13%|‚ñà‚ñé        | 1.31G/9.97G [00:28<02:25, 59.4MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  13%|‚ñà‚ñé        | 1.34G/9.97G [00:29<02:02, 70.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  14%|‚ñà‚ñé        | 1.36G/9.97G [00:29<02:01, 71.2MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  14%|‚ñà‚ñç        | 1.38G/9.97G [00:29<02:06, 68.0MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  14%|‚ñà‚ñç        | 1.39G/9.97G [00:30<02:12, 64.8MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  14%|‚ñà‚ñç        | 1.41G/9.97G [00:30<02:08, 66.8MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  14%|‚ñà‚ñç        | 1.42G/9.97G [00:30<02:25, 58.8MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  14%|‚ñà‚ñç        | 1.44G/9.97G [00:30<02:19, 61.0MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  15%|‚ñà‚ñç        | 1.46G/9.97G [00:31<02:14, 63.4MB/s]\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  15%|‚ñà‚ñç        | 1.47G/9.97G [00:31<02:22, 59.7MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  15%|‚ñà‚ñç        | 1.49G/9.97G [00:31<02:24, 58.7MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  15%|‚ñà‚ñå        | 1.50G/9.97G [00:31<02:09, 65.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  15%|‚ñà‚ñå        | 1.52G/9.97G [00:32<02:00, 70.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  15%|‚ñà‚ñå        | 1.54G/9.97G [00:32<02:07, 66.4MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  16%|‚ñà‚ñå        | 1.55G/9.97G [00:32<02:12, 63.6MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  16%|‚ñà‚ñå        | 1.57G/9.97G [00:32<02:12, 63.4MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  16%|‚ñà‚ñå        | 1.58G/9.97G [00:33<02:14, 62.4MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  16%|‚ñà‚ñå        | 1.60G/9.97G [00:33<02:08, 65.3MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  16%|‚ñà‚ñå        | 1.62G/9.97G [00:33<02:04, 67.1MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  16%|‚ñà‚ñã        | 1.63G/9.97G [00:33<02:02, 67.9MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  17%|‚ñà‚ñã        | 1.65G/9.97G [00:34<02:03, 67.5MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  17%|‚ñà‚ñã        | 1.66G/9.97G [00:34<01:50, 75.2MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  17%|‚ñà‚ñã        | 1.68G/9.97G [00:34<02:01, 68.0MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  17%|‚ñà‚ñã        | 1.70G/9.97G [00:34<01:55, 71.9MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  17%|‚ñà‚ñã        | 1.71G/9.97G [00:34<01:50, 75.0MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  17%|‚ñà‚ñã        | 1.73G/9.97G [00:35<02:01, 67.7MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  17%|‚ñà‚ñã        | 1.74G/9.97G [00:35<02:02, 67.4MB/s]\n",
            "\n",
            "model-00001-of-00003.safetensors:  18%|‚ñà‚ñä        | 1.76G/9.97G [00:35<01:54, 71.7MB/s]\n",
            "\n",
            "model-00003-of-00003.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.98G/1.98G [00:35<00:00, 55.2MB/s]\n",
            "model-00002-of-00003.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.98G/9.98G [02:35<00:00, 64.3MB/s]\n",
            "model-00001-of-00003.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.97G/9.97G [03:00<00:00, 55.2MB/s]\n",
            "\n",
            "Upload 3 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:00<00:00, 60.32s/it] \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Haleshot/Mathmate-7B-dare-ties/commit/e1aa198a23a84a7b031164d70b858c5910ba7809', commit_message='Upload folder using huggingface_hub', commit_description='', oid='e1aa198a23a84a7b031164d70b858c5910ba7809', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title ## Upload model to Hugging Face { display-mode: \"form\" }\n",
        "# @markdown Enter your HF username and the name of Colab secret that stores your [Hugging Face access token](https://huggingface.co/settings/tokens).\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv() # This loads the .env file at the project root\n",
        "\n",
        "username = 'Haleshot' # @param {type:\"string\"}\n",
        "# token = 'HF_TOKEN' # @param {type:\"string\"}\n",
        "\n",
        "token = os.getenv('HF_TOKEN')\n",
        "license = \"apache-2.0\" # @param [\"apache-2.0\", \"cc-by-nc-4.0\", \"mit\", \"openrail\"] {allow-input: true}\n",
        "\n",
        "!pip install -qU huggingface_hub\n",
        "\n",
        "import yaml\n",
        "\n",
        "from huggingface_hub import ModelCard, ModelCardData, HfApi\n",
        "# from google.colab import userdata\n",
        "from jinja2 import Template\n",
        "\n",
        "if branch == \"main\":\n",
        "    template_text = \"\"\"\n",
        "---\n",
        "license: {{ license }}\n",
        "base_model:\n",
        "{%- for model in models %}\n",
        "  - {{ model }}\n",
        "{%- endfor %}\n",
        "tags:\n",
        "- merge\n",
        "- mergekit\n",
        "- lazymergekit\n",
        "{%- for model in models %}\n",
        "- {{ model }}\n",
        "{%- endfor %}\n",
        "---\n",
        "\n",
        "# {{ model_name }}\n",
        "\n",
        "{{ model_name }} is a merge of the following models using [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing):\n",
        "\n",
        "{%- for model in models %}\n",
        "* [{{ model }}](https://huggingface.co/{{ model }})\n",
        "{%- endfor %}\n",
        "\n",
        "## üß© Configuration\n",
        "\n",
        "```yaml\n",
        "{{- yaml_config -}}\n",
        "```\n",
        "\n",
        "## üíª Usage\n",
        "\n",
        "```python\n",
        "!pip install -qU transformers accelerate\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"{{ username }}/{{ model_name }}\"\n",
        "messages = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "    # Create a Jinja template object\n",
        "    jinja_template = Template(template_text.strip())\n",
        "\n",
        "    # Get list of models from config\n",
        "    data = yaml.safe_load(yaml_config)\n",
        "    if \"models\" in data:\n",
        "        models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\n",
        "    elif \"parameters\" in data:\n",
        "        models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\n",
        "    elif \"slices\" in data:\n",
        "        models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\n",
        "    else:\n",
        "        raise Exception(\"No models or slices found in yaml config\")\n",
        "\n",
        "    # Fill the template\n",
        "    content = jinja_template.render(\n",
        "        model_name=MODEL_NAME,\n",
        "        models=models,\n",
        "        yaml_config=yaml_config,\n",
        "        username=username,\n",
        "    )\n",
        "\n",
        "elif branch == \"mixtral\":\n",
        "    template_text = \"\"\"\n",
        "---\n",
        "license: {{ license }}\n",
        "base_model:\n",
        "{%- for model in models %}\n",
        "  - {{ model }}\n",
        "{%- endfor %}\n",
        "tags:\n",
        "- moe\n",
        "- frankenmoe\n",
        "- merge\n",
        "- mergekit\n",
        "- lazymergekit\n",
        "{%- for model in models %}\n",
        "- {{ model }}\n",
        "{%- endfor %}\n",
        "---\n",
        "\n",
        "# {{ model_name }}\n",
        "\n",
        "{{ model_name }} is a Mixture of Experts (MoE) made with the following models using [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing):\n",
        "\n",
        "{%- for model in models %}\n",
        "* [{{ model }}](https://huggingface.co/{{ model }})\n",
        "{%- endfor %}\n",
        "\n",
        "## üß© Configuration\n",
        "\n",
        "```yaml\n",
        "{{- yaml_config -}}\n",
        "```\n",
        "\n",
        "## üíª Usage\n",
        "\n",
        "```python\n",
        "!pip install -qU transformers bitsandbytes accelerate\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"{{ username }}/{{ model_name }}\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True},\n",
        ")\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Explain what a Mixture of Experts is in less than 100 words.\"}]\n",
        "prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "    # Create a Jinja template object\n",
        "    jinja_template = Template(template_text.strip())\n",
        "\n",
        "    # Fill the template\n",
        "    data = yaml.safe_load(yaml_config)\n",
        "    models = [model['source_model'] for model in data['experts']]\n",
        "\n",
        "    content = jinja_template.render(\n",
        "        model_name=MODEL_NAME,\n",
        "        models=models,\n",
        "        yaml_config=yaml_config,\n",
        "        username=username,\n",
        "        license=license\n",
        "    )\n",
        "\n",
        "# Save the model card\n",
        "card = ModelCard(content)\n",
        "card.save('merge/README.md')\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "api = HfApi(token=token)\n",
        "\n",
        "# Upload merge folder\n",
        "api.create_repo(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        ")\n",
        "api.upload_folder(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    folder_path=\"merge\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Mathmate-7B-dare-ties</strong> at: <a href='https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning/runs/69xavjzm' target=\"_blank\">https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning/runs/69xavjzm</a><br/> View project at: <a href='https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning' target=\"_blank\">https://wandb.ai/haleshot-SVKM%27s%20Narsee%20Monjee%20Institute%20of%20Management%20St/Mathmate-stage1-finetuning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240825_162651-69xavjzm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
